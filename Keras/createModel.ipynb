{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('./data/cleaned.csv', encoding='utf-8')\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 1600000/1600000 [00:30<00:00, 52802.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# post process\n",
    "data['clean_text'] = data['clean_text'].progress_map(tokenize)\n",
    "data = data[data.clean_text != 'NC']\n",
    "data.reset_index(inplace=True)\n",
    "data.drop('index', inplace=True, axis=1)\n",
    "# print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(np.array(data['clean_text']), np.array(data['target']),\n",
    "                                                    test_size=0.2)\n",
    "x_test_pure = x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josh\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "0it [00:00, ?it/s]C:\\Users\\josh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  if __name__ == '__main__':\n",
      "1269600it [00:05, 234273.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledSentence(words=['knoww', 'crazi', 'coupl', 'day', 'thank', 'tweet', 'rescu'], tags=['TRAIN_0']), LabeledSentence(words=['ouch', 'least', 'carniv'], tags=['TRAIN_1'])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "317400it [00:00, 347372.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building word2vec matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1269600/1269600 [00:00<00:00, 2360530.79it/s]\n",
      "100%|██████████| 1269600/1269600 [00:00<00:00, 2193451.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36988349, 43740285)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "\n",
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "print(x_train[0:2])\n",
    "x_test = labelizeTweets(x_test, 'TEST')\n",
    "\n",
    "print(\"Building word2vec matrix\")\n",
    "n_dim = 200\n",
    "tweet_w2v = Word2Vec(size=n_dim, min_count=10)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm(x_train)])\n",
    "tweet_w2v.train([x.words for x in tqdm(x_train)], total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tf-idf matrix\n",
      "vocab size:  20216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print(\"Building tf-idf matrix\")\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in x_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print(\"Vocab size: \", len(tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling train vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\josh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "1269600it [01:23, 15296.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling test vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\josh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "317400it [00:20, 15253.48it/s]\n"
     ]
    }
   ],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "print(\"Scaling train vector\")\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "print(\"Scaling test vector\")\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Keras Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making model\n",
      "WARNING:tensorflow:From C:\\Users\\josh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# build keras based model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "print(\"Making model\")\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=200))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "WARNING:tensorflow:From C:\\Users\\josh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/9\n",
      " - 51s - loss: -1.7511e+01 - acc: 0.1024\n",
      "Epoch 2/9\n",
      " - 52s - loss: -1.7647e+01 - acc: 0.0984\n",
      "Epoch 3/9\n",
      " - 50s - loss: -1.7638e+01 - acc: 0.0944\n",
      "Epoch 4/9\n",
      " - 50s - loss: -1.7620e+01 - acc: 0.0931\n",
      "Epoch 5/9\n",
      " - 51s - loss: -1.7601e+01 - acc: 0.0913\n",
      "Epoch 6/9\n",
      " - 49s - loss: -1.7585e+01 - acc: 0.0902\n",
      "Epoch 7/9\n",
      " - 47s - loss: -1.7573e+01 - acc: 0.0908\n",
      "Epoch 8/9\n",
      " - 45s - loss: -1.7566e+01 - acc: 0.0902\n",
      "Epoch 9/9\n",
      " - 51s - loss: -1.7562e+01 - acc: 0.0909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e619115b00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training model\")\n",
    "model.fit(train_vecs_w2v, y_train, epochs=9, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate and Analyze Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "0.10874606175173283\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model\")\n",
    "score = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)\n",
    "print(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TextBlob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "317400it [00:43, 7230.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15982041587901702\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_tweet_sentiment(tweet): \n",
    "        ''' \n",
    "        Utility function to classify sentiment of passed tweet \n",
    "        using textblob's sentiment method \n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text \n",
    "        analysis = TextBlob(tweet) \n",
    "        # set sentiment \n",
    "        if analysis.sentiment.polarity > 0: \n",
    "            return 0\n",
    "        elif analysis.sentiment.polarity == 0: \n",
    "            return 2\n",
    "        else: \n",
    "            return 4\n",
    "\n",
    "print(\"Evaluating TextBlob\")\n",
    "right = 0\n",
    "for i, tweet in tqdm(enumerate(x_test_pure)):\n",
    "    if(get_tweet_sentiment(\" \".join(tweet)) == y_test[i]):\n",
    "        right += 1\n",
    "print(right/len(x_test_pure))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
